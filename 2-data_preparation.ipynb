{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparação dos dados\n",
    "\n",
    "* **Data Cleansing**: Identificar e corrigir inconsistências, valores em falta ou inválidos.\n",
    "* **Data Transformation**: Transformar os dados, se necessário, para uniformizar as variáveis.\n",
    "* **Data Imputation**: Preencher valores em falta com métodos apropriados.\n",
    "* **Data Weighting and Balancing**: Garantir que os dados estão equilibrados, especialmente em casos de classes desiguais.\n",
    "* **Data Filtering**: Selecionar os dados mais relevantes.\n",
    "* **Data Reduction**: Reduzir o volume total de dados, mantendo informação essencial.\n",
    "* **Data Sampling (Records)**: Obter uma amostra representativa dos registos.\n",
    "* **Dimensionality Reduction (Variables)**: Reduzir o número de variáveis, mantendo as mais significativas.\n",
    "* **Data Discretization (Values)**: Agrupar valores contínuos em categorias.\n",
    "* **Data Derivation**: Criar novas variáveis com base nas existentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 2.1 - Bibliotecas Utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar Bibliotecas\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Dataset Original\n",
    "https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler o arquivo CSV \"Life Expectancy Data.csv\" guaradado na pasta data e carregar o ficheiro para um DataFrame designado de \"df_original\".\n",
    "df_original = pd.read_csv('./data/Life Expectancy Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 - Informações Gerais Dataset\n",
    "\n",
    "- Primeiras linhas do dataset\n",
    "- Principais informações do dataset\n",
    "- Estatísticas do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 - Preparaçao Dataset\n",
    "\n",
    "- Limpar os nomes das colunas\n",
    "- Verificar se existem valores nulos\n",
    "- Tratar valores ausentes ou nulos\n",
    "- Confirmar se temos valores duplicados\n",
    "- Confirmar quais valores únicos de cada variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.columns = df_original.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratar valores ausentes para 'GDP' e 'Schooling'\n",
    "df_original['GDP'] = df_original['GDP'].fillna(df_original['GDP'].median())\n",
    "df_original['Schooling'] = df_original['Schooling'].fillna(df_original['Schooling'].median())\n",
    "df_original.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_counts = df_original[\"Country\"].value_counts()\n",
    "print(\"Initial counts of target variable classes:\")\n",
    "print(initial_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 - Ananálise exploratória\n",
    "\n",
    "1. Estatísticas descritivas: Resumo estatístico para variáveis numéricas.\n",
    "2. Distribuição das variáveis: Identificação de padrões ou possíveis outliers.\n",
    "3. Correlação entre variáveis: Análise da relação entre elas, para detectar redundâncias ou dependências.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para categorizar as variáveis de um DataFrame\n",
    "def analyze_variable_types(df):\n",
    "    variable_types = {\n",
    "        \"binary\": [],  # Variáveis binárias (0 ou 1)\n",
    "        \"categorical\": [],  # Variáveis categóricas (discretas)\n",
    "        \"continuous\": [],  # Variáveis contínuas (numéricas)\n",
    "    }\n",
    "\n",
    "    # Analisar cada coluna no DataFrame\n",
    "    for column in df.columns:\n",
    "        unique_values = df[column].dropna().unique()  # Eliminar valores nulos\n",
    "\n",
    "        if set(unique_values).issubset({0, 1, 0.0, 1.0}):\n",
    "            variable_types[\"binary\"].append(column)\n",
    "        elif df[column].dtype in [\"int64\", \"float64\"] and len(unique_values) > 3:\n",
    "            variable_types[\"continuous\"].append(column)\n",
    "        else:\n",
    "            variable_types[\"categorical\"].append(column)\n",
    "\n",
    "    # Output do número de variáveis por tipo\n",
    "    print(f\"\\nNúmero de variáveis binárias: {len(variable_types['binary'])}\")\n",
    "    print(f\"Número de variáveis categóricas: {len(variable_types['categorical'])}\")\n",
    "    print(f\"Número de variáveis contínuas: {len(variable_types['continuous'])}\")\n",
    "\n",
    "    return variable_types\n",
    "\n",
    "\n",
    "# Aplicar a função ao conjunto de dados\n",
    "variable_types = analyze_variable_types(df_original)\n",
    "\n",
    "# Exibir os tipos de variáveis\n",
    "print(\"\\nVariáveis Binárias:\")\n",
    "print(variable_types[\"binary\"])\n",
    "print(\"\\nVariáveis Categóricas:\")\n",
    "print(variable_types[\"categorical\"])\n",
    "print(\"\\nVariáveis Contínuas:\")\n",
    "print(variable_types[\"continuous\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar a variável 'Status' em valores numéricos (1 e 2)\n",
    "df_original['Status'] = df_original['Status'].map({'Developed': 1, 'Developing': 2})\n",
    "\n",
    "# Verificar as transformações realizadas\n",
    "df_original['Status'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar os valores únicos na coluna 'Status'\n",
    "unique_status_values = df_original['Status'].unique()\n",
    "unique_status_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 - Normalização e Variáveis Mais Importantes\n",
    "\n",
    "- Definir o alvo (y) e as características (X)\n",
    "- Normalizar as variáveis contínuas com MinMaxScaler\n",
    "- Obter importâncias das variáveis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alvo (expectativa de vida)\n",
    "y = df_original['Life expectancy']\n",
    "\n",
    "# Características socioeconómicas selecionadas\n",
    "socioeconomic_features = ['GDP', 'Schooling', 'Income composition of resources', 'Alcohol', 'Population']\n",
    "X = df_original[socioeconomic_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalizar as variáveis contínuas com MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = X.copy()\n",
    "\n",
    "# Aplicar a normalização\n",
    "X_scaled[socioeconomic_features] = scaler.fit_transform(X[socioeconomic_features])\n",
    "\n",
    "# Guardar o scaler para uso futuro\n",
    "joblib.dump(scaler, \"./data/scaler.pkl\")\n",
    "\n",
    "# Confirmar a normalização\n",
    "X_scaled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Limpar dados: Remover colunas não numéricas e valores nulos\n",
    "data_cleaned = df_original.drop(['Country', 'Year', 'Status'], axis=1).dropna()\n",
    "\n",
    "# Separar variáveis dependentes e independentes\n",
    "X = data_cleaned.drop('Life expectancy', axis=1)\n",
    "y = data_cleaned['Life expectancy']\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Criar o modelo Random Forest\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Obter importâncias das variáveis\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Criar DataFrame para visualizar as importâncias\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Criar um gráfico de barras para importância das variáveis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(feature_importance['Feature'], feature_importance['Importance'], edgecolor='black')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Variáveis')\n",
    "plt.ylabel('Importância')\n",
    "plt.title('Importância das Variáveis para a Esperança Média de Vida')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.7 - Preparação dos dados de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir os dados em conjunto de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar os dados\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter os arrays em DataFrames ou Series\n",
    "X_train_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "y_train_df = pd.Series(y_train, name='Life expectancy')\n",
    "y_test_df = pd.Series(y_test, name='Life expectancy')\n",
    "\n",
    "# Guardar os dados em ficheiros CSV\n",
    "X_train_df.to_csv('./data/X_train.csv', index=False)\n",
    "X_test_df.to_csv('./data/X_test.csv', index=False)\n",
    "y_train_df.to_csv('./data/y_train.csv', index=False)\n",
    "y_test_df.to_csv('./data/y_test.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MTAD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
